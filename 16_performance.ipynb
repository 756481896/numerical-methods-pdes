{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<table>\n",
    " <tr align=left><td><img align=left src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\">\n",
    " <td>Text provided under a Creative Commons Attribution license, CC-BY. All code is made available under the FSF-approved MIT license. (c) Kyle T. Mandli</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Performance\n",
    "\n",
    "We have not really paid much attention to performance of code beyond basic instruction counting up until now.  These lectures are dedicated to a deeper dive into these issues and what are some important things to keep in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: Matrix-Matrix Multiplication\n",
    "\n",
    "To start our discussion let us consider the algorithm for matrix-matrix multiplication which algorithmically looks like\n",
    "```\n",
    "do i=1:N\n",
    "    do j=1:N\n",
    "        do k=1:N\n",
    "            C[i, j] = C[i, j] + A[i, k] * B[k, j]\n",
    "        end do\n",
    "    end do\n",
    "end do\n",
    "```\n",
    "\n",
    "Consider the follow approaches to this problem:\n",
    "1. Matrix multiplication via a GCC (Fortran) intrinsic.\n",
    "1. Straight forward three-loop multiplication\n",
    "1. Parallelized double-loop using BLAS intrinsic\n",
    "1. BLAS intrinsic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```fortran\n",
    "real function matrix_multiply_test(N,method)\n",
    "\n",
    "    use mod_rand\n",
    "    implicit none\n",
    "    \n",
    "    external DGEMM,DDOT\n",
    "    \n",
    "    double precision :: DDOT\n",
    "    integer, intent(in) :: N,method\n",
    "    integer :: start,finish,count_rate\n",
    "    double precision, dimension(:,:), allocatable :: A,B,C\n",
    "    \n",
    "    ! Local\n",
    "    integer :: i,j,k\n",
    "    \n",
    "    ! Create the random arrays\n",
    "    call init_random_seed()\n",
    "    allocate(A(N,N),B(N,N),C(N,N))\n",
    "    call random_number(A)\n",
    "    call random_number(B)\n",
    "    \n",
    "    ! Start the timer and start multiplying\n",
    "    call system_clock(start,count_rate)\n",
    "    select case(method)\n",
    "        case(1) ! Default method provided as an intrinsic method\n",
    "            C = matmul(A,B)\n",
    "        case(2) ! Simple three loop multiplication\n",
    "            !$OMP parallel do private(j,k)\n",
    "            do i=1,N\n",
    "                do j=1,N\n",
    "                    do k=1,N\n",
    "                        C(i,j) = C(i,j) + A(i,k) * B(k,j)\n",
    "                    enddo\n",
    "                enddo\n",
    "            enddo\n",
    "        case(3) ! OpenMP parallelized double loop\n",
    "            !$OMP parallel do private(j)\n",
    "            do i=1,N\n",
    "                do j=1,N\n",
    "                    C(i,j) = DDOT(N, A(i,:), 1, B(:,j), 1)\n",
    "                enddo\n",
    "            enddo\n",
    "        case(4) ! BLAS Routine call\n",
    "            ! call DGEMM(transa,transb,l,n,m,alpha,a,lda,b,ldb,beta,c,ldc)\n",
    "            call DGEMM('N', 'N', N, N, N, 1.d0, A, N, B, N, 0.d0, C, N)\n",
    "        case default\n",
    "            print *, \"***ERROR*** Invalid multiplication method!\"\n",
    "            matrix_multiply_test = -1\n",
    "            return\n",
    "    end select\n",
    "    call system_clock(finish,count_rate)\n",
    "    \n",
    "    matrix_multiply_test = float(finish - start) / float(count_rate)\n",
    "    \n",
    "end function matrix_multiply_test\n",
    "    \n",
    "program matrix_multiply\n",
    "    \n",
    "    use omp_lib\n",
    "\n",
    "    implicit none\n",
    "    \n",
    "    integer :: N, method, threads\n",
    "    character(len=10) :: input_N, input_method, input_threads\n",
    "    real :: matrix_multiply_test, time\n",
    "    \n",
    "    select case(iargc())\n",
    "        case(0)\n",
    "            N = 1000\n",
    "            method = 1\n",
    "            threads = 1\n",
    "        case(1)\n",
    "            call getarg(1,input_N)\n",
    "            read(input_N,'(I10)') N\n",
    "            method = 1\n",
    "        case(2)\n",
    "            call getarg(1,input_N)\n",
    "            call getarg(2,input_method)\n",
    "            read(input_N,'(I10)') N\n",
    "            read(input_method,'(I10)') method\n",
    "        case(3)\n",
    "            call getarg(1,input_N)\n",
    "            call getarg(2,input_method)\n",
    "            call getarg(3,input_threads)\n",
    "            read(input_N,'(I10)') N\n",
    "            read(input_method,'(I10)') method\n",
    "            read(input_threads,'(I10)') threads\n",
    "        case default\n",
    "            print *, \"***ERROR*** Too many arguments!\"\n",
    "            stop\n",
    "    end select\n",
    "    \n",
    "    !$ call omp_set_num_threads(threads)\n",
    "\n",
    "    time = matrix_multiply_test(N, method)\n",
    "    \n",
    "    print '(\"Timing for \",i5,\"x\",i5,\" matrices: \",f10.5,\" s\")',N,N,time\n",
    "    \n",
    "end program matrix_multiply\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Results\n",
    "\n",
    "Based on $1000 \\times 1000$ matrix-matrix multiply compiled with `gfortran` version 6.3.0 with the compile time flags `-O3 -funroll-loops -finline-functions -fdefault-real-8 -fopenmp`.\n",
    "\n",
    "\n",
    "Method                           | No-Threads            | Threaded\n",
    "---------------------------------|-----------------------|---------------------------\n",
    "Default mat_mult function        |            35.79600 s |                36.19100 s\n",
    "3 loop multiplication            |            39.24700 s |                10.04000 s                   \n",
    "Double loop (internal BLAS)      |             6.80500 s |                 1.76600 s                   \n",
    "BLAS Routine call                |             0.00300 s |                 0.00300 s                   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Architecture Considerations\n",
    "\n",
    "Before we can talk about performance we need to understand a bit about modern computing architectures.  Note that this glossing over a lot of important details as we will focus on only the details that we will specifically deal with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Von Neumann Architecture\n",
    "![image](./images/vonneumann_architecture.png)\n",
    "*Kapooht - Wikipedia Commons*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instruction Pipeline\n",
    "![image](./images/pipeline_1.png)\n",
    "*Cburnett - Wikipedia Commons*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image](./images/memory_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Moore's Law\n",
    "\n",
    "In 1965, Gordon Moore (co-founder of Intel) predicted that the transistor density (and hence the speed) of chips would double every 18 months for the forseeable future. This is know as Mooreâ€™s law This proved remarkably accurate for more than 40 years, see the graphs at. Note that doubling every 18 months means an increase by a factor of 4096 every 14 years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Is Moore's Law Doomed?\n",
    "![image](./images/moores_law.png)\n",
    "*Steve Jurvetson - Wikipedia Commons - https://www.flickr.com/photos/jurvetson/31409423572/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Current Performance Bottlenecks\n",
    "\n",
    " - Transistors can no longer be packed more densely in a single core\n",
    " - Memory is really the bottleneck\n",
    " - Hard limit due to the speed of light\n",
    " - Power consumption and therefore heat dissipation\n",
    " \n",
    "### Solutions?\n",
    " - Many-core technologies\n",
    " - Memory hierarchies\n",
    " - Algorithms that take into account these limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image](./images/memory_single_core.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image](./images/pipeline_2.png)\n",
    "*Cburnett - Wikipedia Commons*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Roof-line Model\n",
    "![image](./images/roofline.png)\n",
    "*Giu.natale - Wikipedia Commons*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Many-Core Architectures\n",
    "\n",
    "![image](./images/kepler_arch.png)\n",
    "![image](./images/kepler_smx.png)\n",
    "*NVIDIA - Kepler GK110/210 White Paper - http://images.nvidia.com/content/pdf/tesla/NVIDIA-Kepler-GK110-GK210-Architecture-Whitepaper.pdf*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallelization\n",
    "\n",
    "Parallelization is one of the primary ways we can increase performance on today's computing architectures.  There are 2+1 major types of parallelization paradigms:\n",
    " - Shared memory - each pipeline can access the memory for the entire problem\n",
    " - Distributed memory - each pipeline can only access part of the memory for the entire problem\n",
    " - Hybrid parallelism - use both..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Shared Memory\n",
    "\n",
    " - Basic construct is a *thread* - each thread has a pipeline and in the simplest case each core runs one thread\n",
    " - OpenMP, CUDA, OpenCL, OpenACC\n",
    " - Single nodes on a cluster, GPU, Xeon Phi, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distributed Memory\n",
    "\n",
    " - Basic contruct is a *process* \n",
    " - Each process is memory local but can communicate to other processes either on the same CPU or across a network\n",
    " - Each process can have multiple threads (hybrid parallelism)\n",
    " - MPI is most common\n",
    " - Clusters, super-computers, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Scalability\n",
    "\n",
    "Measures of parallel performance:\n",
    "\n",
    " - Strong Scaling:  Execution time decreases inversely proportional to the number of processes\n",
    "   - Fixed size problem\n",
    " - Weak Scaling: Execution time remains constant as problem size and processes number are increased proportionally\n",
    "   - Variable size problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### OpenMP\n",
    "\n",
    "OpenMP is defined by a set of *directives* that are put into code which on compilation a compiler can turn into multi-threaded code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```fortran\n",
    "program yeval\n",
    "   \n",
    "   use omp_lib\n",
    "\n",
    "   implicit none\n",
    "\n",
    "   integer(kind=8), parameter :: n = 2**16\n",
    "   integer(kind=4) :: i, nthreads\n",
    "   real(kind=8), dimension(n) :: y\n",
    "   real(kind=8) :: dx, x\n",
    "\n",
    "   ! Specify number of threads to use:\n",
    "   !$ print *, \"How many threads to use? \"\n",
    "   !$ read *, nthreads\n",
    "   !$ call omp_set_num_threads(nthreads)\n",
    "   !$ print \"('Using OpenMP with ',i3,' threads')\", nthreads\n",
    "\n",
    "   dx = 1.d0 / (n+1.d0)\n",
    "\n",
    "   !$omp parallel do private(x) \n",
    "   do i=1, n\n",
    "      x = i * dx\n",
    "      y(i) = exp(x) * cos(x) * sin(x) * sqrt(5.d0 * x + 6.d0)\n",
    "   enddo\n",
    "   !$omp end parallel do\n",
    "\n",
    "   print *, \"Filled vector y of length\", n\n",
    "\n",
    "end program yeval\n",
    "```\n",
    "*Modified from amath 583 - R.J. LeVeque - http://faculty.washington.edu/rjl/classes/am583s2014/notes/openmp.html*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Fine-Grain vs. Coarse-Grain Parallelism\n",
    "\n",
    "Consider the problem of normalizing a vector which requires two steps:\n",
    "1. Compute the norm of the vector, and\n",
    "1. Divide each entry of the vector by the norm.\n",
    "\n",
    "Unfortunately we need to loop over every entry in the vector to compute the norm **before** we can perform the division of each entry.  There are two ways to tackle this problem,\n",
    " - Let the compiler decide what thread takes what entries (fine grain)\n",
    " - Let the programmer explicitly control which entries are handled by each thread (coarse grain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```fortran\n",
    "program fine_grain\n",
    "   \n",
    "    use omp_lib\n",
    "    implicit none\n",
    "    integer :: i, thread_num\n",
    "    integer, parameter :: n = 1000\n",
    " \n",
    "    real(kind=8), dimension(n) :: x, y\n",
    "    real(kind=8) :: norm,ynorm\n",
    " \n",
    "    integer :: nthreads\n",
    "    \n",
    "    ! Specify number of threads to use:\n",
    "    nthreads = 1       ! need this value in serial mode\n",
    "    !$ nthreads = 4    \n",
    "    !$ call omp_set_num_threads(nthreads)\n",
    "    !$ print \"('Using OpenMP with ',i3,' threads')\", nthreads\n",
    "\n",
    "    ! Specify number of threads to use:\n",
    "    !$ call omp_set_num_threads(4)\n",
    " \n",
    "    ! initialize x:\n",
    "    !$omp parallel do \n",
    "    do i=1,n\n",
    "        x(i) = real(i, kind=8)  ! convert to double float\n",
    "    enddo\n",
    "\n",
    "    norm = 0.d0\n",
    "    ynorm = 0.d0\n",
    "\n",
    "    !$omp parallel private(i)\n",
    "\n",
    "    !$omp do reduction(+ : norm)\n",
    "    do i=1,n\n",
    "        norm = norm + abs(x(i))\n",
    "    enddo\n",
    "\n",
    "     !$omp barrier   ! not needed (implicit)\n",
    "\n",
    "    !$omp do reduction(+ : ynorm)\n",
    "    do i=1,n\n",
    "        y(i) = x(i) / norm\n",
    "        ynorm = ynorm + abs(y(i))\n",
    "    enddo\n",
    "    \n",
    "    !$omp end parallel\n",
    "\n",
    "    print *, \"norm of x = \",norm, \"  n(n+1)/2 = \",n*(n+1)/2\n",
    "    print *, 'ynorm should be 1.0:   ynorm = ', ynorm\n",
    "\n",
    "end program fine_grain\n",
    "\n",
    "```\n",
    "*Modified from amath 583 - R.J. LeVeque - http://faculty.washington.edu/rjl/classes/am583s2014/notes/openmp.html*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```fortran\n",
    "program coarse_grain\n",
    "    \n",
    "    use omp_lib\n",
    "    implicit none\n",
    "    integer, parameter :: n = 1000\n",
    "    real(kind=8), dimension(n) :: x,y\n",
    "    real(kind=8) :: norm,norm_thread,ynorm,ynorm_thread\n",
    "    integer :: nthreads, points_per_thread,thread_num\n",
    "    integer :: i,istart,iend\n",
    "\n",
    "    ! Specify number of threads to use:\n",
    "    nthreads = 1       ! need this value in serial mode\n",
    "    !$ nthreads = 4    \n",
    "    !$ call omp_set_num_threads(nthreads)\n",
    "    !$ print \"('Using OpenMP with ',i3,' threads')\", nthreads\n",
    "\n",
    "    ! Determine how many points to handle with each thread.\n",
    "    ! Note that dividing two integers and assigning to an integer will\n",
    "    ! round down if the result is not an integer.  \n",
    "    ! This, together with the min(...) in the definition of iend below,\n",
    "    ! insures that all points will get distributed to some thread.\n",
    "    points_per_thread = (n + nthreads - 1) / nthreads\n",
    "    print *, \"points_per_thread = \",points_per_thread\n",
    "\n",
    "    ! initialize x:\n",
    "    do i=1,n\n",
    "        x(i) = dble(i)  ! convert to double float\n",
    "        enddo\n",
    "\n",
    "    norm = 0.d0\n",
    "    ynorm = 0.d0\n",
    "\n",
    "    !$omp parallel private(i,norm_thread, &\n",
    "    !$omp                  istart,iend,thread_num,ynorm_thread) \n",
    "\n",
    "    thread_num = 0     ! needed in serial mode\n",
    "    !$ thread_num = omp_get_thread_num()    ! unique for each thread\n",
    "\n",
    "    ! Determine start and end index for the set of points to be \n",
    "    ! handled by this thread:\n",
    "    istart = thread_num * points_per_thread + 1\n",
    "    iend = min((thread_num+1) * points_per_thread, n)\n",
    "\n",
    "    !$omp critical\n",
    "    print \"(\"Thread \",i2,\" will take i = \",i6,\" through i = \",i6)\", thread_num, istart, iend\n",
    "    !$omp end critical\n",
    "\n",
    "    norm_thread = 0.d0\n",
    "    do i=istart,iend\n",
    "        norm_thread = norm_thread + abs(x(i))\n",
    "        enddo\n",
    "\n",
    "    ! update global norm with value from each thread:\n",
    "    !$omp critical\n",
    "      norm = norm + norm_thread\n",
    "      print *, \"norm updated to: \",norm\n",
    "    !$omp end critical\n",
    "\n",
    "    ! make sure all have updated norm before proceeding:\n",
    "    !$omp barrier\n",
    "\n",
    "    ynorm_thread = 0.d0\n",
    "    do i=istart,iend\n",
    "        y(i) = x(i) / norm\n",
    "        ynorm_thread = ynorm_thread + abs(y(i))\n",
    "        enddo\n",
    "\n",
    "    ! update global ynorm with value from each thread:\n",
    "    !$omp critical\n",
    "      ynorm = ynorm + ynorm_thread\n",
    "      print *, \"ynorm updated to: \",ynorm\n",
    "    !$omp end critical\n",
    "    !$omp barrier\n",
    "\n",
    "    !$omp end parallel \n",
    "\n",
    "    print *, \"norm of x = \",norm, \"  n(n+1)/2 = \",n*(n+1)/2\n",
    "    print *, 'ynorm should be 1.0:   ynorm = ', ynorm\n",
    "\n",
    "end program coarse_grain\n",
    "```\n",
    "*Modified from amath 583 - R.J. LeVeque - http://faculty.washington.edu/rjl/classes/am583s2014/notes/openmp.html*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MPI\n",
    "\n",
    "*Modified from amath 583 - R.J. LeVeque - http://faculty.washington.edu/rjl/classes/am583s2014/notes/mpi.html*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example - Jacobi Iteration\n",
    "\n",
    "http://faculty.washington.edu/rjl/classes/am583s2014/notes/jacobi1d_omp1.html\n",
    "http://faculty.washington.edu/rjl/classes/am583s2014/notes/jacobi1d_omp2.html\n",
    "http://faculty.washington.edu/rjl/classes/am583s2014/notes/jacobi1d_mpi.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Software Packages\n",
    "\n",
    "### Linear Algebra\n",
    " - BLAS - Basic Lineag Algebra Subroutines\n",
    "   - Level 1: Scalar and vector operations\n",
    "   - Level 2: Matrix-vector operations\n",
    "   - Level 3: Matrix-matrix operations\n",
    " - LAPACK - Linear Algebra Package\n",
    " - ScaLAPACK - Parallel LAPACK\n",
    "\n",
    "### Solving ODEs\n",
    "ODEPACK\n",
    "\n",
    "### Solving PDEs"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
