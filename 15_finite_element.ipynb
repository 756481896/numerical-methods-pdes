{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<table>\n",
    " <tr align=left><td><img align=left src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\">\n",
    " <td>Text provided under a Creative Commons Attribution license, CC-BY. All code is made available under the FSF-approved MIT license. (c) Kyle T. Mandli (and Marc Spiegelman)</td>\n",
    "</table>\n",
    "\n",
    "Much of this material was adapted from the first few chapters of [Larson and Bengzon: The Finite Element Method\n",
    "Theory, Implementation,\n",
    "and Applications](http://link.springer.com/book/10.1007%2F978-3-642-33287-6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Finite Element Methods: an introduction\n",
    "\n",
    "The Finite Element Method (FEM) provides yet another method of transforming continuous PDE's to systems of discrete linear and non-linear equations and is particularly popular in Engineering and Solid Mechanics (although the method is completely general).  Finite Elements has a rich mathematical basis in functional analysis as well a very wide range of specialized applications.  Needless to say, this subject and the accompanying literature is vast.  The purpose of these notes are just to give you a very basic introduction to some of the fundamental ideas in Finite Elements and their implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finite Dimensional Function spaces\n",
    "\n",
    "A key idea in Finite Elements is that we will be approximating continuous functions $f(x)$ as a linear combination of a finite number of basis functions\n",
    "\n",
    "$$\n",
    "    f_h(x) = \\sum_{i=0}^N w_i \\phi_i(x)\n",
    "$$\n",
    "\n",
    "where $\\phi_i(x)$ are the basis functions and $w_i$ are a set of weights.  We've seen similar decompositions for spectral methods. However instead of using global basis functions such as $\\sin(kx)$ or Chebyshev polynomials,  we will (for the most part) use piecewise polynomials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Piecewise Linear Functions in 1-D (P1)\n",
    "\n",
    "![](./images/P1_function_annotated_fig1.1LB.png)\n",
    "\n",
    "As an example, consider the piecewise linear \"connect the dots\" function illustrated above where we have decomposed an interval $I=[x_0,x_5]$ into 5 sub-intervals or \"elements\"  $e_i = [x_{i},x_{i+1}]$ with $i=1\\ldots 4$.  Over each of these intervals, the function $v_h$ is a linear function of $x$ and each segment is continuous at the element edge.  Figure modified from Figure 1.1 of Larson and Bengzon.  \n",
    "\n",
    "While $v_h(x)$ is a continous function that can be evaluated for any value of $x$, it should be clear that there are really only 6 degrees of freedom corresponding to the values at the nodes $\\vec v = [v_0,v_1,v_2,v_3,\\ldots,v_5]$.  By just changing these 6 numbers, we can generate an infinite number of piecewise linear functions on this triangulation.  We will call this finite dimensional space of functions $\\cal V_h$, which is fully described by a mesh of elements, and an interpolant over each element.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Global Basis Functions\n",
    "\n",
    "Given any discrete function space $\\cal V_h$, we should be able to describe any function $v_h\\in \\cal V_h$ in terms of a set of basis functions that span the space.  In the case of a 1-D piecewise linear function space, the global basis functions are given by the \"Hat Functions\"  \n",
    "\n",
    "$$\n",
    "\\phi_i(x) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "(x - x_{i-1})/h_{i-1}, & x\\in e_{i-1} \\\\\n",
    "(x_i - x)/h_{i}, & x\\in e_{i} \\\\\n",
    "0, & \\text{otherwise} \\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Which have the important property that $\\phi_i(x_j) = \\delta_{ij}$.  A typical hat function is shown in Figure 1.2 from Larson and Bengson.\n",
    "![](./images/P1_Hat_function_fig1.2LB.png)\n",
    "\n",
    "While it's not entirely obvious,  we can construct all piecewise linear functions in $\\cal V_h$ as a linear combination of these hat functions, i.e. \n",
    "\n",
    "$$\n",
    "f_h(x) = \\sum_{i=0}^5 w_i\\phi_i(x)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Local Basis Functions\n",
    "\n",
    "To make this a bit clearer it is worth looking at how these basis functions interpolate $v$ over a single element.\n",
    "\n",
    "![](./images/P1LocalBases.png)\n",
    "\n",
    "> **Figure *3*: Definition of Global and Local Basis Functions**\n",
    "\n",
    "Consider just the portion of $v(x)$ that lies in element $e_i$.  Clearly the only global basis functions that have support in element $e_i$ are $\\phi_i(x)$ and $\\phi_{i+1}(x)$.  We claim that over this element\n",
    "$$\n",
    "v(x) = v_i\\phi_i(x) + v_{i+1}\\phi_{i+1}(x)\\quad\\text{for}\\quad x\\in [x_i,x_{i+1}]\n",
    "$$\n",
    "\n",
    "This is clearly true at the nodes because of the property that $\\phi_i(x_j) = \\delta_{ij}$.  \n",
    "\n",
    "Between the nodes, we expect that $\\phi_i$ and $\\phi_{i+1}$ provide linear interpolation.  If we just consider the *local basis functions* which are the parts of $\\phi_i$ and $\\phi_{i+1}$ in element $e_i$\n",
    "\n",
    "\\begin{align}\n",
    "  N_0(x) & = (x_{i+1} - x)/(x_{i+1} - x_i)\\\\\n",
    "  N_1(x) & = (x - x_i)/(x_{i+1} - x_i)\\\\\n",
    "\\end{align}\n",
    "\n",
    "it becomes clear that these are just the Lagrange Polynomials of order 1 that interpolate any linear function over the interval $[x_i,x_{i+1}]$.   Just to check note that if $x* = x_i + th_i$ where $t$ is the fractional distance across the element with lenght $h_i = x_{i+1} - x_i$, then\n",
    "\\begin{align}\n",
    "  N_0(x*) & = (x_{i+1} - x_i -t h_i)/h_i = (1 -t)\\\\\n",
    "  N_1(x*) & = (x_i + th_i - x_i)/h_i = t\\\\\n",
    "\\end{align}\n",
    "\n",
    "so that within $e_i$\n",
    "$$ \n",
    "   v(x) = v_i N_0(x) + v_{i+1}N_1(x) = v_i(1 - t)  + v_{i+1} t\n",
    "$$\n",
    "with $t\\in[0,1]$\n",
    "\n",
    "Thus our full function $v(x)$ over the full domain $\\Omega = [x_0,x_5]$ is just the union of these local linear pieces.  Throughout this introduction we will often move between the global view and the element view depending on which frame of reference is more convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Higher order polynomial Function Spaces\n",
    "\n",
    "The space of piecewise linear functions is often referred to as $P_1$ but there are other choices.  For example, the space of piecewise quadratic functions ($P_2$) has a set of local basis functions that are simply the Lagrange Polynomials of order 2 and require three degrees of freedom per node.  The global basis functions are simply constructed out of the local ones.  The Figure shows a subset of basis functions for $P_2$ which are different for degrees of freedom at the centers and edges of elements (original figure from [Introduction to Finite Element Methods](http://hplgit.github.io/INF5620/doc/pub/sphinx-fem)\n",
    "\n",
    "![](http://hplgit.github.io/INF5620/doc/pub/sphinx-fem/_images/mpl_fe_basis_p2_4e_lab.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpolation of a function $f(x)$ onto $\\cal V$\n",
    "\n",
    "Given a set of basis functions $\\phi_i(x)$ for a function space $\\cal V$, it should be clear that not all functions can be represented exactly, without error in $\\cal V$.  For example, given a smooth function $f(x)$ the question arises how to construct approximations $f_h(x) \\in \\cal V$ that minimize some measure of the error.  Actually there are multiple approaches.  The simplest approximation is simply the *interpolation* of $f$ onto $\\cal V$ which we will denote $\\pi f$ and define as the function $\\pi f \\in \\cal V$ such that\n",
    "\n",
    "$$\n",
    "    \\pi f (x) = \\sum_{i=0}^N f(x_i)\\phi(x)\n",
    "$$\n",
    "\n",
    "Given the interpolating properties of the Lagrange polynomials, it should be clear that\n",
    "\n",
    "$$\n",
    "\\pi f(x_j) = f(x_j)\n",
    "$$\n",
    "\n",
    "i.e. the two functions have zero error at the degrees of freedom. \n",
    "Figure 1.3 from Larson and Bengzon shows the linear interpolant of a function $f$ onto a single element over the interval $[x_0,x_1]$.\n",
    "\n",
    "![](./images/f_interpolation_P1_01.png)\n",
    "\n",
    "Figure 1.4 from Larson and Bengzon shows the linear interpolant of a function $f$ onto a piecewise linear function space $\\cal V$.\n",
    "\n",
    "![](./images/f_interpolation_P1_02.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Interpolation Errors\n",
    "\n",
    "In general there will usually be an error between $f(x)$ and its interpolant $\\pi f(x)$. Quantifying this error though usually requires defining some norm and as usual there are multiple choices.  As it turns out however,  a useful norm in finite elements are the $L^2$ norm of a function over a domain $\\Omega$\n",
    "\n",
    "$$\n",
    "    ||f||_{L^2} = \\left[ \\int_\\Omega f^2 dx\\right]^{1/2}\n",
    "$$\n",
    "\n",
    "and its related norm\n",
    "\n",
    "$$\n",
    "    ||f||_{L^2(I)} = \\left[ \\int_I f^2 dx\\right]^{1/2}\n",
    "$$\n",
    "\n",
    "over a sub-interval $I$ (usually an element).\n",
    "\n",
    "Given these definitions, and the behavior of polynomial interpolation it is relatively straightforward to put the following estimates on the interpolation error\n",
    "\n",
    "* Interpolation error over a single P1 linear element\n",
    "\n",
    "$$ \n",
    "|| f - \\pi f||_{L^2(I)} \\leq C h^2||f''||_{L^2(I)}\n",
    "$$\n",
    "\n",
    "where $I = [x_i,x{i+1}]$, $h = x_{i+1} - x{i}$, C is a constant, and $||f''||_{L^2(I)}$ is a measure of the curvature of the function over the interval.  If $h=0$ or the function is linear over the interval, then the error is zero.\n",
    "\n",
    "* Interpolation error over the whole domain\n",
    "\n",
    "For a piecewise polynomial function, the errors in the interpolant onto $\\cal V$ are simply the summation of the errors over the element i.e. \n",
    "\n",
    "$$ \n",
    "|| f - \\pi f||^2_{L^2} \\leq C \\sum_{i=0}^N h_i^4||f''||^2_{L^2(I_i)}\n",
    "$$\n",
    "\n",
    "### Convergence\n",
    "\n",
    "Given these error estimates, it is clear that the interpolant converges to the continuous function in the $L^2$ norm as $h\\rightarrow 0$ which makes sense as we expect the interpolant to be a better approximation to $f$ for finer and finer meshes. For $P_1$ functions, the error converges as $h^2$,  higher order interpolants of *smooth* functions should converge more rapidly if the higher-order piecewise polynomial is a good approximation to the continuous function $f$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $L_2$ Projection of a function $f(x)$ onto $\\cal V$\n",
    "\n",
    "While the interpolant $\\pi f$ converges to $f$ in the $L^2$ norm as $h\\rightarrow 0$, it turns out that for fixed $h$, it is not the function in $\\cal V$ with the smallest $L^2$ error.  For that problem we need the orthogonal $L^2$ projection $P_h f$ which turns out to be the unique function $v_h\\in\\cal V$ that minimizes $||f - v_h||_{L^2}$.  \n",
    "Figure 1.6 from Larson and Bengzon show the orthogonal projection of $f$ onto $\\cal V$ for $f(x) = x\\sin(\\pi x)$\n",
    "\n",
    "![](./images/f_projection_P1_01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Relationship to Least-Squares projection in linear algebra\n",
    "\n",
    "Before we develop the machinery for calculating  the orthogonal $L^2$ projection of $f$ onto $\\cal V$, $P_h f$,it is probably worth reviewing, briefly the related orthongal projection problem from linear algebra. \n",
    "\n",
    "![](./images/projection_figure.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a 2-dimensional subspace $\\cal S$ of $R^3$ spanned by two linearly independent 3-D vectors $\\boldsymbol{a}_1$ and $\\boldsymbol{a}_2$. i.e.\n",
    "\n",
    "$$\n",
    "\\cal S = \\mathrm{span}<\\boldsymbol{a}_1,\\boldsymbol{a}_2>\n",
    "$$ \n",
    "\n",
    "Let $\\boldsymbol{b}$ be a third vector not in $\\cal S$.  The problem is to find the point $\\boldsymbol{p} \\in \\cal S$ that minimizes the Euclidian 2-norm of the error $\\boldsymbol{e} = \\boldsymbol{b} - \\boldsymbol{p}$ where $||\\boldsymbol{e}||_2 = \\sqrt{\\boldsymbol{e}^T\\boldsymbol{e}}$\n",
    "\n",
    "Inspection of the figure shows geometrically that the shortest error will be to find the point $\\boldsymbol{p} = x_1\\boldsymbol{a}_1 + x_2\\boldsymbol{a}_2$ such that $\\boldsymbol{e}$ is orthogonal to the plane $\\cal S$.  If we define a matrix \n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "    \\boldsymbol{a}_1 & \\boldsymbol{a}_2\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "whose columns are the vectors $\\boldsymbol{a}_1$ and $\\boldsymbol{a}_2$, then $\\boldsymbol{p} = A\\boldsymbol{x}$ where \n",
    "\n",
    "$$\n",
    "\\boldsymbol{x} = \\begin{bmatrix}\n",
    "    x_1\\\\\n",
    "    x_2\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now for $\\boldsymbol{e}$ to be orthogonal to every vector in $\\cal S$ it is sufficient for it to orthogonal to the basis vectors $\\boldsymbol{a}_1$ and $\\boldsymbol{a}_2$.  Thus the requirement of orthogonality is that \n",
    "\n",
    "$$\n",
    "    \\boldsymbol{a}_i^T\\boldsymbol{e} = 0\n",
    "$$\n",
    "\n",
    "for $i=1,2$.  This is equivalent to the statement $A^T\\boldsymbol{e}=\\boldsymbol{0}$  (or that the error is in the Left null space of $A$).\n",
    "\n",
    "But\n",
    "\\begin{align}\n",
    "    A^T\\boldsymbol{e} & = A^T(\\boldsymbol{b} - \\boldsymbol{p}) \\\\\n",
    "                      & = A^T(\\boldsymbol{b} - A\\boldsymbol{x}) \\\\\n",
    "                      & = \\boldsymbol{0}\n",
    "\\end{align}\n",
    "\n",
    "or the Least-squares solution requires solving the Normal Equations\n",
    "\n",
    "$$\n",
    "    A^TA\\boldsymbol{x} = A^T\\boldsymbol{b}\n",
    "$$\n",
    "\n",
    "(although the numerically sensible  way to solve this is to actually use the QR factorization which is another application of orthogonal projection)\n",
    "\n",
    "Given a solution $\\boldsymbol{x}$, the projection is simply $\\boldsymbol{p} = A\\boldsymbol{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Orthogonal Projection of a function onto a discrete Function Space\n",
    "\n",
    "If you understand the previous problem from linear algebra,  you'll realize that orthogonal projection of a function onto a discrete function space is essentially the same problem with slightly different machinery.  \n",
    "\n",
    "The problem can be stated:\n",
    "\n",
    "Given a continuous function $f(x)$ and discrete function space $\\cal V_h$,  find the function $P_h f\\in \\cal V_h$ that minimizes $ || f - P_h f||_{L^2}$.  \n",
    "\n",
    "Again, we can construct the problem geometrically.  First if $P_h f \\in \\cal V_h$ then we can write it in terms of the basis functions of $\\cal V_h$ i.e.\n",
    "\n",
    "$$\n",
    "    P_h f(x)  = \\sum_{j=0}^N w_j\\phi_j(x)\n",
    "$$\n",
    "\n",
    "Next we construct the residual (or error) as\n",
    "\n",
    "$$ \n",
    "    r(x) = f(x) - P_h f(x) \n",
    "$$\n",
    "and require that the residual is \"orthogonal\" to every function $v_h\\in \\cal V_h$ where we define orthogonality of functions with respect to the $L^2$ norm in the usual way.  i.e. we say that $f$ and $g$ are orthogonal functions if\n",
    "\n",
    "$$\n",
    "    \\int_\\Omega fg dx = 0\n",
    "$$\n",
    "\n",
    "Therefore for $r$ to be orthogonal to every function $v_h\\in \\cal V_h$, it is only required that it be orthogonal to every basis function of $\\cal V_h$.  For a finite dimensional function space the problem becomes find $P_h f \\in \\cal V_h$ such that\n",
    "\n",
    "$$ \n",
    "    \\int_\\Omega \\phi_i(x) r(x) dx = 0\n",
    "$$ \n",
    "for all $i=0,\\ldots,N$.  \n",
    "\n",
    "Using our definitions for $r(x)$ and $P_h f(x)$, the problem becomes,  find the vector of weights $\\boldsymbol{w} = [ w_0, w_1,\\ldots, w_N]^T$ such that\n",
    "\n",
    "$$\n",
    "    \\int_\\Omega \\phi_i(x)\\sum_{j=0}^N w_j\\phi_j(x) dx = \\int_\\Omega \\phi_i(x) f(x) dx\n",
    "$$ \n",
    "for all $i=0,\\ldots,N$.  \n",
    "\n",
    "Now the RHS of this equation is a set of $N$ numbers that each describes the $L^2$ inner product of $f$ with each basis function. Defining\n",
    "\n",
    "$$\n",
    "    \\tilde{f}_i = \\int_\\Omega \\phi_i(x) f(x) dx\\quad i=0,\\ldots,N\n",
    "$$\n",
    "the RHS assembles into a vector $\\tilde{\\boldsymbol{f}}\\in R^{N+1}$.  \n",
    "\n",
    "The LHS is a bit more complicated but we can interchange summation and integration to rewrite the LHS as\n",
    "\n",
    "$$\n",
    "\\int_\\Omega \\phi_i(x)\\sum_{j=0}^N w_j\\phi_j(x) dx = \\sum_{j=0}^N w_j\\int_\\Omega \\phi_i(x)\\phi_j(x) dx\n",
    "$$\n",
    "\n",
    "Now the final definite integrals are another set of doubly indexed numbers\n",
    "\n",
    "$$\n",
    "    M_{ij} = \\int_\\Omega \\phi_i(x)\\phi_j(x) dx\n",
    "$$\n",
    "\n",
    "which define the entries of the \"Mass Matrix\" $M$.  \n",
    "\n",
    "Thus the projection problem actually reduces to a Linear algebra \n",
    "$$\n",
    "    \\sum_{j=0}^N M_{ij}w_j = \\tilde{f}_i\n",
    "$$\n",
    "or in matrix vector notation,  Solve\n",
    "$$\n",
    "    M\\boldsymbol{w} = \\tilde{\\boldsymbol{f}}\n",
    "$$\n",
    "for $\\boldsymbol{w}$ then $P_h f(x)$ is defined by the weights and basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](./images/FEM_projection_assembly.png)\n",
    "\n",
    "Figure 9 showing all the components needed to assemble the mass matrix $M$ and the load vector $\\tilde{\\boldsymbol{f}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Structure of the Mass Matrix\n",
    "\n",
    "To understand the structure of the mass matrix, it is useful to examine the above figure.\n",
    "\n",
    "Given a 1-D interval with $N$ elements,  the Mass Matrix $M$ is a $N+1\\times N+1$ matrix who's components are given by \n",
    "\n",
    "$$\n",
    "    M_{ij} = \\int_\\Omega \\phi_i(x)\\phi_j(x) dx\n",
    "$$\n",
    "\n",
    "If the basis functions were Orthonormal (e.g. sines and cosines or Chebyshev polynomials) the Mass Matrix would be equal to the Identity $I$.  However,  if the basis functions are hat functions examination of figure 9 shows that it should be sparse as Basis functions that don't share the same support should have $M_{ij}=0$.  For the case of 1-D problems in $P_1$,  it should be clear that the Mass Matrix should actually be tri-diagonal  as the only non-zero elements will arise from the interaction of a basis function $\\phi_i$ and it's two nearest neighbors $\\phi_{i-1}$ and $\\phi_{i+1}$\n",
    "\n",
    "In particular,  the $i$th row of $M$ will have three non-zero entries\n",
    "\n",
    "\\begin{align}\n",
    "    M_{i,i-1} &= \\int_\\Omega \\phi_{i}\\phi_{i-1} dx = \\int_{x_{i-1}}^{x_i} \\phi_{i}\\phi_{i-1} dx \\\\\n",
    "    M_{i,i} &= \\int_\\Omega \\phi_{i}\\phi_{i} dx = \\int_{x_{i-1}}^{x_{i+1}} \\phi_{i}\\phi_{i} dx \\\\\n",
    "    M_{i,i+1} &= \\int_\\Omega \\phi_{i}\\phi_{i+1} dx = \\int_{x_i}^{x_{i+1}} \\phi_{i}\\phi_{i+1} dx \\\\\n",
    "\\end{align}\n",
    "\n",
    "Note that the first term is contributed entirely by element $e_{i-1}$, the last term by element $e_i$ and the diagonal entry actually can be split into two parts\n",
    "\n",
    "$$\n",
    "    M_{i,i} = \\int_{x_{i-1}}^{x_{i}} \\phi_{i}\\phi_{i} dx + \\int_{x_{i}}^{x_{i+1}} \\phi_{i}\\phi_{i} dx\n",
    "$$\n",
    "\n",
    "corresponding to the contributions from both elements $e_{i-1}$ and $e_i$.  For simple  $P_1$ hat functions we can actually calculate these entries analytically (although for more complicated problems or the load matrix we might want to use numerical quadrature).  \n",
    "\n",
    "If we just consider the first term and use the definitions of $\\phi_{i-1}$ and $\\phi_i$\n",
    "\n",
    "$$\n",
    "    M_{i,i-1} = \\int_{x_{i-1}}^{x_i} \\frac{x_{i} - x}{h_{i-1}} \\frac{x - x_{i-1}}{h_{i-1}} dx \n",
    "$$\n",
    "\n",
    "which is straightforward to integrate,  but is easier to do if we first make the affine transformation\n",
    "\n",
    "$$\n",
    " x(t) = x_{i-1} + t h_{i-1}\n",
    "$$\n",
    "\n",
    "where again $t\\in [0,1]$ is the fractional distance across element $e{i-1}$ ($t$ is also known as the local element coordinate).  Given this transformation and $dx = h_{i-1} dt$,  the above integral simplifies to\n",
    "\n",
    "$$\n",
    "        M_{i,i-1} = h_{i-1}\\int_{0}^{1} ( 1- t) t dt = h_{i-1} \\left.\\left( \\frac{t^2}{2} - \\frac{t^3}{3}\\right)\\right|_0^1 = \\frac{h_{i-1}}{6}\n",
    "$$ \n",
    "\n",
    "Using the same approach we can evaluate the remaining terms\n",
    "\\begin{align}\n",
    "M_{i,i} &= h_{i-1}\\int_{0}^{1} t^2 dt  + h_i\\int_{0}^{1} (1 -t)^2 dt = \\frac{h_{i-1}}{3} + \\frac{h_i}{3}\\\\\n",
    "\n",
    "M_{i,i+1} & = h_i\\int_{0}^{1} t(1-t) dt = \\frac{h_i}{6}\n",
    "\\end{align}\n",
    "\n",
    "so the stencil for one row of the mass matrix is simply\n",
    "\n",
    "$$\n",
    "M_i = \\begin{bmatrix}\n",
    "   \\frac{h_{i-1}}{6}  & \\frac{h_{i-1}}{3} + \\frac{h_i}{3} & \\frac{h_i}{6}\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and the Global Mass matrix looks like\n",
    "$$\n",
    "    M = \\begin{bmatrix}\n",
    "    \\frac{h_0}{3}  & \\frac{h_0}{6} & & & & \\\\\n",
    "    \\frac{h_0}{6}  & \\frac{h_0}{3} + \\frac{h_1}{3} & \\frac{h_1}{6} & & & \\\\\n",
    "    &\\frac{h_1}{6}  & \\frac{h_1}{3} + \\frac{h_2}{3} & \\frac{h_2}{6}  & & \\\\\n",
    "    & & \\ddots & \\ddots & \\ddots & \\\\\n",
    "    & & &   &  \\frac{h_N}{6} & \\frac{h_N}{3}\n",
    "    \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Mechanics:  Element Wise Assembly \n",
    "\n",
    "While it is possible to assemble the global mass matrix as described above,  a better approach is to loop over each element and calculate the local contribution of each element to the global mass matrix.  To begin with we can decompose $M$ into a sum of simpler block matrices\n",
    "\n",
    "$$\n",
    "M = \\begin{bmatrix}\n",
    "\\frac{h_0}{3}  & \\frac{h_0}{6} & &  \\\\\n",
    "\\frac{h_0}{6}  & \\frac{h_0}{3}  & & \\\\ \n",
    "&&& \\\\\n",
    "&&& \\\\\n",
    "&&& \\\\\n",
    "&&& \n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "&&& \\\\\n",
    "& \\frac{h_1}{3}  & \\frac{h_1}{6}   & \\\\ \n",
    "& \\frac{h_1}{6}  & \\frac{h_1}{3}  &  \\\\\n",
    "&&& \\\\\n",
    "&&& \\\\\n",
    "&&& \n",
    "\\end{bmatrix} + \\ldots +\n",
    "\\begin{bmatrix}\n",
    "&&& \\\\\n",
    "&&& \\\\\n",
    "&&& \\\\\n",
    "&&& \\\\\n",
    "& & \\frac{h_N}{3}  & \\frac{h_N}{6} \\\\\n",
    "& &\\frac{h_N}{6}  & \\frac{h_N}{3}  \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "M = M^{e_0} + M^{e_1} + \\ldots + M^{e_N}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "  M^{e_i} = \\frac{h_i}{6}\\begin{bmatrix}\n",
    "  2 & 1\\\\\n",
    "  1 & 2\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "  \n",
    "  is the local element Mass Matrix for element $e_i$.  Examination of Figure 3 shows that we can describe the element mass matrix entirely in terms of the local basis functions $N_0(t)$ and $N_1(t)$ in the local coordinate system as\n",
    "  \n",
    "$$\n",
    "  M^{e_i} = h_i\\begin{bmatrix}\n",
    "  \\int_0^1 N_0^2 dt & \\int_0^1 N_0N_1 dt \\\\\n",
    "  \\int_0^1 N_0N_1 dt & \\int_0^1 N_1^2 dt \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and each of these entries correspond to parts (or all) of entries in the global mass matrix\n",
    "\n",
    "$$\n",
    "  M^{e_i} = \\begin{bmatrix}\n",
    "  M_{i,i} & M_{i,i+1} \\\\\n",
    "  M_{i+1,i} & M_{i+1,i+1} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So the overall algorithm is to \n",
    "* Loop over elements $i=0,\\ldots,N$\n",
    "* Calculate the local Mass Matrix $M^{e_i}$\n",
    "* Insert/add into the Global Mass Matrix\n",
    "\n",
    "A similar process is used to assemble the load vector $\\tilde{\\boldsymbol{f}}$ by \n",
    "* Loop over elements $i=0,\\ldots,N$\n",
    "* Calculate the element contribution to the load vector\n",
    "\n",
    "$$\n",
    "    \\tilde{\\boldsymbol{f}}^{e_i} = \\begin{bmatrix}\n",
    "        \\int_{x_{i-1}}^{x_i} N_0(x)f(x) dx \\\\\n",
    "        \\int_{x_{i-1}}^{x_i} N_1(x)f(x) dx \n",
    "        \\end{bmatrix} = \\begin{bmatrix}\n",
    "        \\tilde{f}_i \\\\\n",
    "        \\tilde{f}_{i+1} \n",
    "        \\end{bmatrix}\n",
    "$$\n",
    "* Insert/Add into the Global Load Vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convergence/Optimality ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
